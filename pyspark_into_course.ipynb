{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "\n",
    "## Course Overview\n",
    "\n",
    "This notebook provides a comprehensive introduction to PySpark, the Python API for Apache Spark. We'll cover:\n",
    "- What is Apache Spark?\n",
    "- Setting up a Spark session\n",
    "- Working with DataFrames\n",
    "- Data manipulation and transformations\n",
    "- Basic analytics and machine learning\n",
    "- Performance and distributed computing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "Before running this notebook, ensure you have PySpark installed:\n",
    "```\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importing SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Introduction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Version:\", spark.version)\n",
    "print(\"PySpark is ready to use!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "PySpark offers multiple ways to create DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# From a list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# From a dictionary\n",
    "data_dict = {\n",
    "    \"Name\": [\"David\", \"Eve\"],\n",
    "    \"City\": [\"New York\", \"San Francisco\"],\n",
    "    \"Salary\": [75000, 85000]\n",
    "}\n",
    "df_dict = spark.createDataFrame(list(map(list, zip(*data_dict.values()))), list(data_dict.keys()))\n",
    "df_dict.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading Data from Different Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reading CSV (replace with your file path)\n",
    "# df_csv = spark.read.csv('path/to/your/file.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Example with sample data\n",
    "sales_data = spark.createDataFrame([\n",
    "    (\"Laptop\", \"Electronics\", 1000, 50),\n",
    "    (\"Phone\", \"Electronics\", 500, 100),\n",
    "    (\"Book\", \"Media\", 20, 200),\n",
    "    (\"Tablet\", \"Electronics\", 300, 75)\n",
    "], [\"Product\", \"Category\", \"Price\", \"Stock\"])\n",
    "\n",
    "sales_data.show()\n",
    "\n",
    "# Print schema\n",
    "sales_data.printSchema()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select and filter\n",
    "electronics = sales_data.filter(sales_data.Category == \"Electronics\")\n",
    "print(\"Electronics Products:\")\n",
    "electronics.show()\n",
    "\n",
    "# Add a new column\n",
    "sales_with_total = sales_data.withColumn(\"Total_Value\", F.col(\"Price\") * F.col(\"Stock\"))\n",
    "print(\"\\nSales with Total Value:\")\n",
    "sales_with_total.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregations and Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group by and aggregate\n",
    "category_summary = sales_data.groupBy(\"Category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"Stock\").alias(\"Total_Stock\"),\n",
    "        F.avg(\"Price\").alias(\"Average_Price\")\n",
    "    )\n",
    "\n",
    "print(\"Category Summary:\")\n",
    "category_summary.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window function to rank products within category\n",
    "window_spec = Window.partitionBy(\"Category\").orderBy(F.col(\"Price\").desc())\n",
    "\n",
    "ranked_products = sales_data.withColumn(\n",
    "    \"Price_Rank\", \n",
    "    F.dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"Ranked Products:\")\n",
    "ranked_products.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic Machine Learning with PySpark\n",
    "\n",
    "A simple example of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Prepare data for ML\n",
    "ml_data = spark.createDataFrame([\n",
    "    (1, 2, 10),\n",
    "    (2, 3, 15),\n",
    "    (3, 4, 20),\n",
    "    (4, 5, 25)\n",
    "], [\"feature1\", \"feature2\", \"label\"])\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"feature1\", \"feature2\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "training_data = assembler.transform(ml_data)\n",
    "\n",
    "# Create and train linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance and Distributed Computing\n",
    "\n",
    "PySpark is designed for large-scale data processing:\n",
    "- Lazy evaluation\n",
    "- Distributed computing\n",
    "- Optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate lazy evaluation\n",
    "large_data = spark.range(0, 10000000)\n",
    "filtered_data = large_data.filter(large_data.id % 2 == 0)\n",
    "result = filtered_data.count()\n",
    "\n",
    "print(f\"Number of even numbers: {result}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Closing the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Always good practice to stop the Spark session\n",
    "spark.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Further Learning\n",
    "\n",
    "To continue learning PySpark:\n",
    "- Explore Spark's documentation\n",
    "- Practice with real-world big data scenarios\n",
    "- Learn advanced ML and streaming capabilities\n",
    "\n",
    "Recommended resources:\n",
    "- Apache Spark official documentation\n",
    "- Online big data and distributed computing courses\n",
    "- Data engineering and big data processing books"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}